{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn the physician policy - ie, pi(a|s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/rl_train_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('../data/rl_val_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/rl_test_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features (state vector) and labels (action taken) out of the dataframe for train \n",
    "# and val sets\n",
    "def preproc(df_in, iv_bins = 5):\n",
    "    df = df_in.copy()\n",
    "    actions_raw = df[['iv_input', 'vaso_input']].values\n",
    "    keep_arr = np.loadtxt('../data/state_features.txt', dtype=str)\n",
    "    df = df[keep_arr]\n",
    "    actions_proc = (iv_bins*actions_raw[:, 0] + actions_raw[:, 1]).astype(int)\n",
    "    hist = np.histogram(actions_proc, 25)\n",
    "    actions_proc = pd.get_dummies(actions_proc).values\n",
    "    #print(hist) just to check\n",
    "    return df.values, actions_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sample(batch_size, features, labels):\n",
    "    idx = np.random.choice(np.arange(len(features)), batch_size)\n",
    "    return (np.vstack(features[idx]), np.vstack(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat, train_labels = preproc(train_data)\n",
    "val_feat, val_labels = preproc(val_data)\n",
    "test_feat, test_labels = preproc(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length = len(train_feat[0])\n",
    "batch_size = 64\n",
    "num_actions = 25\n",
    "num_steps = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo - reduce network size\n",
    "class PolicyModel():\n",
    "    def __init__(self):\n",
    "        self.input_feat = tf.placeholder(tf.float32, shape = [None, feature_length])\n",
    "        self.labels = tf.placeholder(tf.float32, shape = [None, num_actions])\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.fc_1 = tf.contrib.layers.fully_connected(self.input_feat, 64, activation_fn=tf.nn.relu)\n",
    "        self.bn_1 = tf.contrib.layers.batch_norm(self.fc_1, center=True, scale=True, is_training=self.phase)\n",
    "#         self.fc_2 = tf.contrib.layers.fully_connected(self.bn_1 , 256, activation_fn=tf.nn.relu)    \n",
    "#         self.bn_2 = tf.contrib.layers.batch_norm(self.fc_2, center=True, scale=True, is_training=self.phase)\n",
    "#         self.fc_3 = tf.contrib.layers.fully_connected(self.bn_2 , 128, activation_fn=tf.nn.relu)\n",
    "#         self.bn_3 = tf.contrib.layers.batch_norm(self.fc_3, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_4 = tf.contrib.layers.fully_connected(self.bn_1 , 64, activation_fn=tf.nn.relu)\n",
    "        self.bn_4 = tf.contrib.layers.batch_norm(self.fc_4, center=True, scale=True, is_training=self.phase)\n",
    "        \n",
    "        self.logits = tf.contrib.layers.fully_connected(self.bn_4 , num_actions, activation_fn=None)\n",
    "        self.output = tf.nn.softmax(self.logits)\n",
    "        self.reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        self.reg_constant = 0.1 \n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.labels, 1), tf.argmax(self.output, 1)),'float32'))\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.labels)) + self.reg_constant*sum(self.reg_losses)\n",
    "\n",
    "        \n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.train_step = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prints out accuracy on the relevant dataset and returns the policy. \n",
    "# This is the probability of taking each action in the action space from that state\n",
    "\n",
    "def get_policy(dataset,sess, mdl):\n",
    "\n",
    "    if dataset == 'train':\n",
    "        features, labels = train_feat,train_labels\n",
    "    elif dataset == 'val':\n",
    "        features, labels = val_feat,val_labels\n",
    "    elif dataset == 'test':\n",
    "        features, labels = test_feat,test_labels\n",
    "\n",
    "    \n",
    "    op = np.zeros((len(features), num_actions))\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    j = 0\n",
    "    while (j < len(features)):\n",
    "        feat = None\n",
    "        lbls = None\n",
    "        if len(features) - j < batch_size:\n",
    "            feat = features[j:-1]\n",
    "            lbls = labels[j:-1]\n",
    "        else:\n",
    "            feat = features[j:j+batch_size]\n",
    "            lbls = labels[j:j+batch_size]\n",
    "        feat = feat.reshape(len(feat), feature_length)\n",
    "        lbls = lbls.reshape(len(lbls), num_actions)\n",
    "        if j%10000 == 0: print('Processing val set indx: ', j )\n",
    "        softmax, accuracy, loss = sess.run([mdl.output, mdl.accuracy, mdl.loss], feed_dict={mdl.input_feat : feat, mdl.phase: 0, mdl.labels: lbls, mdl.phase: 0})\n",
    "        total_acc += accuracy\n",
    "        op[j:j+len(feat)] = softmax\n",
    "        if len(features) - j < batch_size:\n",
    "            j = len(features)\n",
    "        else: j+=batch_size\n",
    "        final_acc = total_acc/(len(op)/batch_size)\n",
    "        total_loss += loss\n",
    "    return op, final_acc, total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    mdl = PolicyModel()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "    config.allow_soft_placement = True  # Enable manual control\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "        net_loss = 0\n",
    "        net_accuracy = 0.0\n",
    "        print('Starting training!')\n",
    "        for i in range(num_steps):\n",
    "            feat, labels = batch_sample(batch_size, train_feat, train_labels)\n",
    "            \n",
    "            _, loss, accuracy = sess.run([mdl.train_step, mdl.loss, mdl.accuracy], feed_dict={mdl.input_feat : feat, mdl.labels: labels, mdl.phase: 1})\n",
    "            \n",
    "            net_loss += loss\n",
    "            net_accuracy += accuracy\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                av_loss = net_loss/1000.0\n",
    "                av_accuracy = net_accuracy/1000.0\n",
    "                print(\"Step: \", i, \"Average loss is: \", av_loss, \"Average accuracy is: \", av_accuracy)\n",
    "                net_loss = 0.0\n",
    "                net_accuracy = 0.0\n",
    "            \n",
    "            if i % 5000 == 0:\n",
    "                print \"Test on validation set\"\n",
    "                _, val_acc, val_loss = get_policy('val', sess, mdl)\n",
    "                print('Val set accuracy, loss: ', val_acc, val_loss)\n",
    "                \n",
    "        # Commented out for now\n",
    "        # train_policy, train_acc = get_policy('train')\n",
    "        print \"Finished, getting final accuracy\"\n",
    "        val_policy, val_acc, val_loss = get_policy('val', sess, mdl)\n",
    "        test_policy, _, _ = get_policy('test',sess, mdl)\n",
    "    print('Val set accuracy, loss: ', val_acc, val_loss)\n",
    "    return val_policy, test_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.013116776315789473, 1259.7755255699158)\n",
      "('Step: ', 1000, 'Average loss is: ', 2.0802375934123991, 'Average accuracy is: ', 0.31106250000000002)\n",
      "('Step: ', 2000, 'Average loss is: ', 1.7987357392311096, 'Average accuracy is: ', 0.33792187499999998)\n",
      "('Step: ', 3000, 'Average loss is: ', 1.7654804812669753, 'Average accuracy is: ', 0.35026562500000002)\n",
      "('Step: ', 4000, 'Average loss is: ', 1.7371502479314804, 'Average accuracy is: ', 0.356375)\n",
      "('Step: ', 5000, 'Average loss is: ', 1.728486176609993, 'Average accuracy is: ', 0.35635937499999998)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.33115808823587078, 687.07190573215485)\n",
      "('Step: ', 6000, 'Average loss is: ', 1.7163822487592697, 'Average accuracy is: ', 0.35684375000000002)\n",
      "('Step: ', 7000, 'Average loss is: ', 1.6976348477602006, 'Average accuracy is: ', 0.36159374999999999)\n",
      "('Step: ', 8000, 'Average loss is: ', 1.691508987545967, 'Average accuracy is: ', 0.36635937499999999)\n",
      "('Step: ', 9000, 'Average loss is: ', 1.687176973581314, 'Average accuracy is: ', 0.365828125)\n",
      "('Step: ', 10000, 'Average loss is: ', 1.6788323541879655, 'Average accuracy is: ', 0.36796875000000001)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.31274671052631581, 766.03627932071686)\n",
      "('Step: ', 11000, 'Average loss is: ', 1.6745273842811585, 'Average accuracy is: ', 0.36796875000000001)\n",
      "('Step: ', 12000, 'Average loss is: ', 1.6678103184700013, 'Average accuracy is: ', 0.36831249999999999)\n",
      "('Step: ', 13000, 'Average loss is: ', 1.6652761453390121, 'Average accuracy is: ', 0.37081249999999999)\n",
      "('Step: ', 14000, 'Average loss is: ', 1.6613902133703231, 'Average accuracy is: ', 0.37193749999999998)\n",
      "('Step: ', 15000, 'Average loss is: ', 1.6582305915355682, 'Average accuracy is: ', 0.36845312499999999)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.34930340557506212, 653.75511634349823)\n",
      "('Step: ', 16000, 'Average loss is: ', 1.6583579243421556, 'Average accuracy is: ', 0.37365625000000002)\n",
      "('Step: ', 17000, 'Average loss is: ', 1.64929041659832, 'Average accuracy is: ', 0.37106250000000002)\n",
      "('Step: ', 18000, 'Average loss is: ', 1.6466398972272873, 'Average accuracy is: ', 0.37318750000000001)\n",
      "('Step: ', 19000, 'Average loss is: ', 1.6536956386566162, 'Average accuracy is: ', 0.37293749999999998)\n",
      "('Step: ', 20000, 'Average loss is: ', 1.6507487127780913, 'Average accuracy is: ', 0.37278125000000001)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.35061919504874633, 657.71765184402466)\n",
      "('Step: ', 21000, 'Average loss is: ', 1.6409473180770875, 'Average accuracy is: ', 0.37462499999999999)\n",
      "('Step: ', 22000, 'Average loss is: ', 1.6399696595668793, 'Average accuracy is: ', 0.3778125)\n",
      "('Step: ', 23000, 'Average loss is: ', 1.6325043289661407, 'Average accuracy is: ', 0.37765625000000003)\n",
      "('Step: ', 24000, 'Average loss is: ', 1.6382397434711455, 'Average accuracy is: ', 0.37507812499999998)\n",
      "('Step: ', 25000, 'Average loss is: ', 1.6413753859996796, 'Average accuracy is: ', 0.37484374999999998)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.35644833594560621, 650.12006890773773)\n",
      "('Step: ', 26000, 'Average loss is: ', 1.6300238615274429, 'Average accuracy is: ', 0.37878125000000001)\n",
      "('Step: ', 27000, 'Average loss is: ', 1.6352037385702134, 'Average accuracy is: ', 0.377)\n",
      "('Step: ', 28000, 'Average loss is: ', 1.6302178710699082, 'Average accuracy is: ', 0.37814062500000001)\n",
      "('Step: ', 29000, 'Average loss is: ', 1.6285549474954606, 'Average accuracy is: ', 0.38)\n",
      "('Step: ', 30000, 'Average loss is: ', 1.6278623861074448, 'Average accuracy is: ', 0.37917187499999999)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.34868662925927263, 661.15503835678101)\n",
      "('Step: ', 31000, 'Average loss is: ', 1.6287418127059936, 'Average accuracy is: ', 0.37712499999999999)\n",
      "('Step: ', 32000, 'Average loss is: ', 1.6330458726882935, 'Average accuracy is: ', 0.37618750000000001)\n",
      "('Step: ', 33000, 'Average loss is: ', 1.6234512648582458, 'Average accuracy is: ', 0.38060937500000003)\n",
      "('Step: ', 34000, 'Average loss is: ', 1.6194059137105943, 'Average accuracy is: ', 0.38415624999999998)\n",
      "Finished, getting final accuracy\n",
      "('Processing val set indx: ', 0)\n",
      "('Processing val set indx: ', 0)\n",
      "('Processing val set indx: ', 40000)\n",
      "('Val set accuracy, loss: ', 0.35282991489297466, 664.99152565002441)\n"
     ]
    }
   ],
   "source": [
    "val_policy, test_policy = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  save the learned policy as a numpy array with the columns as icustayid, bloc, iv input, vaso input,\n",
    "#  action index (of 25), and probability distribution over actions ( this is 25 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_data = val_data[['icustayid', 'bloc', 'iv_input', 'vaso_input']].values\n",
    "val_actions = (5*val_data['iv_input'].values + val_data['vaso_input']).values.astype(int)\n",
    "val_pickle = np.concatenate((v_data, val_actions.reshape(len(val_actions), 1), val_policy), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_data = test_data[['icustayid', 'bloc', 'iv_input', 'vaso_input']].values\n",
    "test_actions = (5*test_data['iv_input'].values + test_data['vaso_input']).values.astype(int)\n",
    "test_pickle = np.concatenate((t_data, test_actions.reshape(len(test_actions), 1), test_policy), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"val_policy.p\", \"wb\") as f:\n",
    "    pickle.dump(val_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"test_policy.p\", \"wb\") as f:\n",
    "    pickle.dump(test_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
