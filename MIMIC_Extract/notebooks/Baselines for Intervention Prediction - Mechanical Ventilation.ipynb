{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils')\n",
    "from simple_impute import simple_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVENTION = 'vent'\n",
    "RANDOM = 0\n",
    "MAX_LEN = 240\n",
    "SLICE_SIZE = 6\n",
    "GAP_TIME = 6\n",
    "PREDICTION_WINDOW = 4\n",
    "OUTCOME_TYPE = 'all'\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_KEY = {'ONSET': 0, 'CONTROL': 1, 'ON_INTERVENTION': 2, 'WEAN': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = '../data/all_hourly_data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf(DATAFILE,'vitals_labs')\n",
    "Y = pd.read_hdf(DATAFILE,'interventions')\n",
    "static = pd.read_hdf(DATAFILE,'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[[INTERVENTION]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Shape of X : ', X.shape\n",
    "print 'Shape of Y : ', Y.shape\n",
    "print 'Shape of static : ', static.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split, Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(static.reset_index(), test_size=0.2, \n",
    "                                       random_state=RANDOM, stratify=static['mort_hosp'])\n",
    "split_train_ids, val_ids = train_test_split(train_ids, test_size=0.125, \n",
    "                                            random_state=RANDOM, stratify=train_ids['mort_hosp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and Standardization of Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = simple_imputer(X,train_ids['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(x):# normalize\n",
    "    mins = x.min()\n",
    "    maxes = x.max()\n",
    "    x_std = (x - mins) / (maxes - mins)\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_time_since_measurement(x):\n",
    "    idx = pd.IndexSlice\n",
    "    x = np.where(x==100, 0, x)\n",
    "    means = x.mean()\n",
    "    stds = x.std()\n",
    "    x_std = (x - means)/stds\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "X_std = X_clean.copy()\n",
    "X_std.loc[:,idx[:,'mean']] = X_std.loc[:,idx[:,'mean']].apply(lambda x: minmax(x))\n",
    "X_std.loc[:,idx[:,'time_since_measured']] = X_std.loc[:,idx[:,'time_since_measured']].apply(lambda x: std_time_since_measurement(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.columns = X_std.columns.droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization of Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(age):\n",
    "    if age > 10 and age <= 30: \n",
    "        cat = 1\n",
    "    elif age > 30 and age <= 50:\n",
    "        cat = 2\n",
    "    elif age > 50 and age <= 70:\n",
    "        cat = 3\n",
    "    else: \n",
    "        cat = 4\n",
    "    return cat\n",
    "\n",
    "def categorize_ethnicity(ethnicity):\n",
    "    if 'AMERICAN INDIAN' in ethnicity:\n",
    "        ethnicity = 'AMERICAN INDIAN'\n",
    "    elif 'ASIAN' in ethnicity:\n",
    "        ethnicity = 'ASIAN'\n",
    "    elif 'WHITE' in ethnicity:\n",
    "        ethnicity = 'WHITE'\n",
    "    elif 'HISPANIC' in ethnicity:\n",
    "        ethnicity = 'HISPANIC/LATINO'\n",
    "    elif 'BLACK' in ethnicity:\n",
    "        ethnicity = 'BLACK'\n",
    "    else: \n",
    "        ethnicity = 'OTHER'\n",
    "    return ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gender, first_careunit, age and ethnicity for prediction\n",
    "static_to_keep = static[['gender', 'age', 'ethnicity', 'first_careunit', 'intime']]\n",
    "static_to_keep.loc[:, 'intime'] = static_to_keep['intime'].astype('datetime64').apply(lambda x : x.hour)\n",
    "static_to_keep.loc[:, 'age'] = static_to_keep['age'].apply(categorize_age)\n",
    "static_to_keep.loc[:, 'ethnicity'] = static_to_keep['ethnicity'].apply(categorize_ethnicity)\n",
    "static_to_keep = pd.get_dummies(static_to_keep, columns = ['gender', 'age', 'ethnicity', 'first_careunit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge time series and static data\n",
    "X_merge = pd.merge(X_std.reset_index(), static_to_keep.reset_index(), on=['subject_id','icustay_id','hadm_id'])\n",
    "# add absolute time feature\n",
    "abs_time = (X_merge['intime'] + X_merge['hours_in'])%24\n",
    "X_merge.insert(4, 'absolute_time', abs_time)\n",
    "X_merge.drop('intime', axis=1, inplace=True)\n",
    "X_merge = X_merge.set_index(['subject_id','icustay_id','hadm_id','hours_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_std, X_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_matrix(x):\n",
    "    zeros = np.zeros((MAX_LEN, x.shape[1]-4))\n",
    "    x = x.values\n",
    "    x = x[:(MAX_LEN), 4:]\n",
    "    zeros[0:x.shape[0], :] = x\n",
    "    return zeros\n",
    "\n",
    "def create_y_matrix(y):\n",
    "    zeros = np.zeros((MAX_LEN, y.shape[1]-4))\n",
    "    y = y.values\n",
    "    y = y[:,4:]\n",
    "    y = y[:MAX_LEN, :]\n",
    "    zeros[:y.shape[0], :] = y\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(X_merge.reset_index().groupby('subject_id').apply(create_x_matrix)))\n",
    "y = np.array(list(Y.reset_index().groupby('subject_id').apply(create_y_matrix)))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = np.array(list(X_merge.reset_index().groupby('subject_id').apply(lambda x: x.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pd.Series(X_merge.reset_index()['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X tensor shape: \", x.shape)\n",
    "print(\"Y tensor shape: \", y.shape)\n",
    "print(\"lengths shape: \", lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.where(keys.isin(train_ids['subject_id']))[0]\n",
    "test_indices = np.where(keys.isin(test_ids['subject_id']))[0]\n",
    "train_static = train_ids\n",
    "split_train_indices = np.where(keys.isin(split_train_ids['subject_id']))[0]\n",
    "val_indices = np.where(keys.isin(val_ids['subject_id']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x[split_train_indices]\n",
    "Y_train = y[split_train_indices]\n",
    "X_test = x[test_indices]\n",
    "Y_test = y[test_indices]\n",
    "X_val = x[val_indices]\n",
    "Y_val = y[val_indices]\n",
    "lengths_train = lengths[split_train_indices]\n",
    "lengths_val = lengths[val_indices]\n",
    "lengths_test = lengths[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training size: \", X_train.shape[0])\n",
    "print(\"Validation size: \", X_val.shape[0])\n",
    "print(\"Test size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3d_tensor_slices(X_tensor, Y_tensor, lengths):\n",
    "\n",
    "    num_patients = X_tensor.shape[0]\n",
    "    timesteps = X_tensor.shape[1]\n",
    "    num_features = X_tensor.shape[2]\n",
    "    X_tensor_new = np.zeros((lengths.sum(), SLICE_SIZE, num_features + 1))\n",
    "    Y_tensor_new = np.zeros((lengths.sum()))\n",
    "\n",
    "    current_row = 0\n",
    "    \n",
    "    for patient_index in range(num_patients):\n",
    "        x_patient = X_tensor[patient_index]\n",
    "        y_patient = Y_tensor[patient_index]\n",
    "        length = lengths[patient_index]\n",
    "\n",
    "        for timestep in range(length - PREDICTION_WINDOW - GAP_TIME - SLICE_SIZE):\n",
    "            x_window = x_patient[timestep:timestep+SLICE_SIZE]\n",
    "            y_window = y_patient[timestep:timestep+SLICE_SIZE]\n",
    "            x_window = np.concatenate((x_window, np.expand_dims(y_window,1)), axis=1)\n",
    "            result_window = y_patient[timestep+SLICE_SIZE+GAP_TIME:timestep+SLICE_SIZE+GAP_TIME+PREDICTION_WINDOW]\n",
    "            result_window_diff = set(np.diff(result_window))\n",
    "            #if 1 in result_window_diff: pdb.set_trace()\n",
    "            gap_window = y_patient[timestep+SLICE_SIZE:timestep+SLICE_SIZE+GAP_TIME]\n",
    "            gap_window_diff = set(np.diff(gap_window))\n",
    "\n",
    "            #print result_window, result_window_diff\n",
    "\n",
    "            if OUTCOME_TYPE == 'binary':\n",
    "                if max(gap_window) == 1:\n",
    "                    result = None\n",
    "                elif max(result_window) == 1:\n",
    "                    result = 1\n",
    "                elif max(result_window) == 0:\n",
    "                    result = 0\n",
    "                if result != None:\n",
    "                    X_tensor_new[current_row] = x_window\n",
    "                    Y_tensor_new[current_row] = result\n",
    "                    current_row += 1\n",
    "\n",
    "            else: \n",
    "                if 1 in gap_window_diff or -1 in gap_window_diff:\n",
    "                    result = None\n",
    "                elif (len(result_window_diff) == 1) and (0 in result_window_diff) and (max(result_window) == 0):\n",
    "                    result = CHUNK_KEY['CONTROL']\n",
    "                elif (len(result_window_diff) == 1) and (0 in result_window_diff) and (max(result_window) == 1):\n",
    "                    result = CHUNK_KEY['ON_INTERVENTION']\n",
    "                elif 1 in result_window_diff: \n",
    "                    result = CHUNK_KEY['ONSET']\n",
    "                elif -1 in result_window_diff:\n",
    "                    result = CHUNK_KEY['WEAN']\n",
    "                else:\n",
    "                    result = None\n",
    "\n",
    "                if result != None:\n",
    "                    X_tensor_new[current_row] = x_window\n",
    "                    Y_tensor_new[current_row] = result\n",
    "                    current_row += 1\n",
    "\n",
    "    X_tensor_new = X_tensor_new[:current_row,:,:]\n",
    "    Y_tensor_new = Y_tensor_new[:current_row]\n",
    "\n",
    "    return X_tensor_new, Y_tensor_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = make_3d_tensor_slices(X_train, Y_train, lengths_train)\n",
    "x_val, y_val = make_3d_tensor_slices(X_val, Y_val, lengths_val)\n",
    "x_test, y_test = make_3d_tensor_slices(X_test, Y_test, lengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_classes = label_binarize(y_train, classes=range(NUM_CLASSES))\n",
    "y_val_classes = label_binarize(y_val, classes=range(NUM_CLASSES))\n",
    "y_test_classes = label_binarize(y_test, classes=range(NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, Y_train, X_test, Y_test, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('shape of x_train: ', x_train.shape)\n",
    "print('shape of x_val: ', x_val.shape)\n",
    "print('shape of x_test: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_col = 17 #static_to_keep.shape[1] - 1\n",
    "time_series_col = 124 #X_merge.shape[1] - static_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_static(x):\n",
    "    x_static = x[:,0,time_series_col:x.shape[2]-1]\n",
    "    x_timeseries = np.reshape(x[:,:,:time_series_col],(x.shape[0], -1))\n",
    "    x_int = x[:,:,-1]\n",
    "    x_concat = np.concatenate((x_static, x_timeseries, x_int), axis=1)\n",
    "    return x_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate hourly features\n",
    "x_train_concat = remove_duplicate_static(x_train)\n",
    "x_val_concat = remove_duplicate_static(x_val)\n",
    "x_test_concat = remove_duplicate_static(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_concat.shape)\n",
    "print(x_val_concat.shape)\n",
    "print(x_test_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "np.random.seed(RANDOM)\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l2']),\n",
    "    'solver': Choice(['sag']),\n",
    "    'max_iter': Choice([100, 200]),\n",
    "    'class_weight': Choice(['balanced']),\n",
    "    'multi_class': Choice(['multinomial']),\n",
    "    'random_state': Choice([RANDOM])\n",
    "})\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "        \n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 200),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "    'class_weight': Choice(['balanced']),\n",
    "    'random_state': Choice([RANDOM])\n",
    "\n",
    "})\n",
    "RF_hyperparams_list = RF_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_train, X_val, X_test):\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "        M = model(**hyperparams)\n",
    "        M.fit(X_train, y_train)\n",
    "        s = roc_auc_score(y_val_classes, M.predict_proba(X_val),average='macro')\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_train, X_val, X_test)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_train, X_val, X_test):\n",
    "    best_M = model(**best_hyperparams)\n",
    "    best_M.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    y_pred  = best_M.predict_proba(X_test)\n",
    "    auc   = roc_auc_score(y_test_classes, y_pred, average=None)\n",
    "    aucmacro = roc_auc_score(y_test_classes, y_pred, average='macro')\n",
    "    \n",
    "    return best_M, best_hyperparams, auc, aucmacro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_name, model, hyperparams_list in [('RF', RandomForestClassifier, RF_hyperparams_list), \n",
    "                                            ('LR', LogisticRegression, LR_hyperparams_list)]:\n",
    "    if model_name not in results: results[model_name] = {}\n",
    "\n",
    "    print(\"Running model %s \" % (model_name))\n",
    "    results[model_name] = run_basic(\n",
    "        model, hyperparams_list, x_train_concat, x_val_concat, x_test_concat)\n",
    "    print(\"Final results for model %s \" % (model_name))\n",
    "    print(results[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, RepeatVector, Lambda\n",
    "from keras.layers import Input, Conv2D, Conv1D, Conv3D, MaxPooling2D, MaxPooling1D\n",
    "from keras.layers import Concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight = dict(zip(range(len(class_weight)), class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "inputs = Input(shape=input_shape)\n",
    "model = Conv1D(64, kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv2')(inputs)\n",
    "\n",
    "model = (MaxPooling1D(pool_size=3, strides=1))(model)\n",
    "\n",
    "model2 = Conv1D(64, kernel_size=4,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv3')(inputs)\n",
    "\n",
    "model2 = MaxPooling1D(pool_size=3, strides=1)(model2)\n",
    "\n",
    "model3 = Conv1D(64, kernel_size=5,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv4')(inputs)\n",
    "\n",
    "model3 = MaxPooling1D(pool_size=3, strides=1)(model3)\n",
    "\n",
    "models = [model, model2, model3]\n",
    "\n",
    "full_model = keras.layers.concatenate(models)\n",
    "full_model = Flatten()(full_model)\n",
    "full_model = Dense(128, activation='relu')(full_model)\n",
    "full_model = Dropout(DROPOUT)(full_model)\n",
    "full_model = Dense(NUM_CLASSES, activation='softmax')(full_model)\n",
    "\n",
    "full_model = keras.models.Model(input=inputs, outputs=full_model)\n",
    "\n",
    "full_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=.0005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "full_model.fit(x_train, y_train_classes,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          class_weight=class_weight,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_cnn = full_model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average=None))\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average='macro'))\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "KEEP_PROB = 0.8\n",
    "REGULARIZATION = 0.001\n",
    "NUM_HIDDEN = [512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceLabelling:\n",
    "\n",
    "    def __init__(self, data, target, dropout_prob, reg, num_hidden=[256], class_weights=[1,1,1,1]):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.reg = reg\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = len(num_hidden)\n",
    "        self.num_classes = len(class_weights)\n",
    "        self.attn_length = 0\n",
    "        self.class_weights = class_weights\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def make_rnn_cell(self,\n",
    "                      attn_length=0,\n",
    "                      base_cell=tf.nn.rnn_cell.BasicLSTMCell,\n",
    "                      state_is_tuple=True):\n",
    "\n",
    "        attn_length = self.attn_length\n",
    "        input_dropout = self.dropout_prob\n",
    "        output_dropout = self.dropout_prob\n",
    "\n",
    "        cells = []\n",
    "        for num_units in self._num_hidden:\n",
    "            cell = base_cell(num_units, state_is_tuple=state_is_tuple)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_dropout, output_keep_prob=output_dropout)\n",
    "            cells.append(cell)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=state_is_tuple)\n",
    "\n",
    "        if attn_length > 0:\n",
    "            sys.path.insert(0, 'attention')\n",
    "            import attention_cell_wrapper_single\n",
    "            cell = attention_cell_wrapper_single.AttentionCellWrapper(\n",
    "                cell, attn_length, input_size=int(self.data.get_shape().as_list()[2]), state_is_tuple=state_is_tuple)\n",
    "            print cell\n",
    "        return cell\n",
    "\n",
    "\n",
    "    # predictor for slices\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "\n",
    "        cell = self.make_rnn_cell\n",
    "\n",
    "        # Recurrent network.\n",
    "        output, final_state = tf.nn.dynamic_rnn(cell,\n",
    "            self.data,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        with tf.variable_scope(\"model\") as scope:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            # final weights\n",
    "            num_classes = self.num_classes\n",
    "            weight, bias = self._weight_and_bias(self._num_hidden[-1], num_classes)\n",
    "    \n",
    "            # flatten + sigmoid\n",
    "            if self.attn_length > 0: \n",
    "                logits = tf.matmul(final_state[0][-1][-1], weight) + bias\n",
    "            else: \n",
    "                logits = tf.matmul(final_state[-1][-1], weight) + bias\n",
    "\n",
    "            prediction = tf.nn.softmax(logits)\n",
    "            \n",
    "            return logits, prediction\n",
    "\n",
    "        \n",
    "    @lazy_property\n",
    "    def cross_ent(self):\n",
    "        predictions = self.prediction[0]\n",
    "        real = tf.cast(tf.squeeze(self.target), tf.int32)\n",
    "\n",
    "        class_weight = tf.expand_dims(tf.cast(self.class_weights, tf.int32), axis=0)\n",
    "        print(\"class_weights\", class_weight)\n",
    "        one_hot_labels = tf.cast(tf.one_hot(real, depth=self.num_classes), tf.int32)\n",
    "        weight_per_label = tf.cast(tf.transpose(tf.matmul(one_hot_labels, tf.transpose(class_weight))), tf.float32) #shape [1, batch_size]\n",
    "\n",
    "        xent = tf.multiply(weight_per_label, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=predictions, name=\"xent_raw\")) #shape [1, batch_size]\n",
    "        loss = tf.reduce_mean(xent) #shape 1\n",
    "        ce = loss\n",
    "        l2 = self.reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        ce += l2\n",
    "        return ce\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.0003\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cross_ent)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        prediction = tf.argmax(self.prediction[1], 1)\n",
    "        real = tf.cast(self.target, tf.int32)\n",
    "        prediction = tf.cast(prediction, tf.int32)\n",
    "        mistakes = tf.not_equal(real, prediction)\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        mistakes = tf.reduce_sum(mistakes, reduction_indices=0)\n",
    "        total = 128\n",
    "        mistakes = tf.divide(mistakes, tf.to_float(total))\n",
    "        return mistakes\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def summaries(self):\n",
    "        tf.summary.scalar('loss', tf.reduce_mean(self.cross_ent))\n",
    "        tf.summary.scalar('error', self.error)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "# if attn_length > 0:\n",
    "#     # weights file initialized\n",
    "#     weight_file = 'weights.txt'\n",
    "#     with open(weight_file, 'a') as the_file:\n",
    "#         pass\n",
    "\n",
    "with tf.Session(config = config) as sess, tf.device('/cpu:0'):\n",
    "    _, length, num_features = x_train.shape\n",
    "    num_data_cols = num_features\n",
    "    print \"num features\", num_features\n",
    "    print \"num_data cols\", num_data_cols\n",
    "\n",
    "    # placeholders\n",
    "    data = tf.placeholder(tf.float32, [None, length, num_data_cols])\n",
    "    target = tf.placeholder(tf.float32, [None])\n",
    "    dropout_prob = tf.placeholder(tf.float32)\n",
    "    reg = tf.placeholder(tf.float32)\n",
    "\n",
    "    # initialization\n",
    "    model = VariableSequenceLabelling(data, target, dropout_prob, reg, num_hidden=NUM_HIDDEN, class_weights=class_weight)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initialized Variables...')\n",
    "\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    dp = KEEP_PROB\n",
    "    rp = REGULARIZATION\n",
    "    train_samples = x_train.shape[0]\n",
    "    indices = range(train_samples)\n",
    "    num_classes = NUM_CLASSES\n",
    "    \n",
    "    # for storing results\n",
    "    test_data = x_test\n",
    "    val_data = x_val\n",
    "\n",
    "    val_aucs = []\n",
    "    test_aucs = []\n",
    "    val_aucs_macro = []\n",
    "    test_aucs_macro = []\n",
    "    \n",
    "    epoch = -1\n",
    "\n",
    "    print('Beginning Training...')\n",
    "\n",
    "    while (epoch < 3 or max(np.diff(early_stop[-3:])) > 0):\n",
    "        epoch += 1\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        num_batches = train_samples/batch_size\n",
    "        for batch_index in range(num_batches):\n",
    "\n",
    "            sample_indices = indices[batch_index*batch_size:batch_index*batch_size+batch_size]\n",
    "            batch_data = x_train[sample_indices, :, :num_data_cols]\n",
    "            batch_target = y_train[sample_indices]\n",
    "            _, loss = sess.run([model.optimize, model.cross_ent], {data: batch_data, target: batch_target, dropout_prob: dp, reg: rp})\n",
    "\n",
    "            # write train accuracy to log files every 10 batches\n",
    "            #if batch_index % 2000 == 0:\n",
    "            #    loss, prediction, error = sess.run([model.cross_ent, model.prediction, model.error], {data: batch_data, target: batch_target, dropout_prob: dp, reg: rp})\n",
    "            #    #train_writer.add_summary(summaries, global_step=epoch*batch_index)\n",
    "            #    print('Epoch {:2d} Batch {:2d}'.format(epoch+1, batch_index))\n",
    "            #    print('Loss = ', np.mean(loss))\n",
    "            #    print('Error = ', error)\n",
    "\n",
    "        cur_val_preds = sess.run(model.prediction, {data: x_val, target: y_val, dropout_prob: 1, reg: rp}) \n",
    "        val_preds = cur_val_preds[1]\n",
    "        \n",
    "        cur_test_preds = sess.run(model.prediction, {data: x_test, target: y_test, dropout_prob: 1, reg: rp}) \n",
    "        test_preds = cur_test_preds[1]\n",
    "\n",
    "        val_auc_macro = roc_auc_score(y_val_classes, val_preds, average='macro')\n",
    "        test_auc_macro = roc_auc_score(y_test_classes, test_preds, average='macro')\n",
    "        val_aucs_macro.append(val_auc_macro)\n",
    "        test_aucs_macro.append(test_auc_macro)\n",
    "\n",
    "        val_auc = roc_auc_score(y_val_classes, val_preds, average=None)\n",
    "        test_auc = roc_auc_score(y_test_classes, test_preds, average=None)\n",
    "        val_aucs.append(val_auc)\n",
    "        test_aucs.append(test_auc)\n",
    "        \n",
    "        if isinstance(val_aucs_macro[-1], dict):\n",
    "            early_stop = [val_auc_macro for val_auc_macro in val_aucs_macro]\n",
    "        else: \n",
    "            early_stop = val_aucs_macro\n",
    "\n",
    "\n",
    "        print \"Val AUC = \", val_auc\n",
    "        print \"Test AUC = \", test_auc\n",
    "\n",
    "\n",
    "    if isinstance(val_aucs_macro[-1], dict):\n",
    "        best_epoch = np.argmax(np.array([val_auc_macro for val_auc_macro in val_aucs_macro]))\n",
    "    else: \n",
    "        best_epoch = np.argmax(val_aucs_macro)\n",
    "\n",
    "    best_val_auc = val_aucs[best_epoch]\n",
    "    best_test_auc = test_aucs[best_epoch]\n",
    "    best_test_auc_macro = test_aucs_macro[best_epoch]\n",
    "\n",
    "    print 'Best Test AUC: ', best_test_auc, 'at epoch ', best_epoch\n",
    "    print 'Best Test AUC Macro: ', best_test_auc_macro, 'at epoch ', best_epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
